{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of dataset in characters:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split: str) -> tuple[torch.tensor, torch.tensor]:\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.8416, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zeros() received an invalid combination of arguments - got (tuple, dtype=module), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[267], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode(m\u001b[38;5;241m.\u001b[39mgenerate(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[43m)\u001b[49m, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros() received an invalid combination of arguments - got (tuple, dtype=module), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(13)\n",
    "\n",
    "class BigramLangugeModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor = None) -> tuple[torch.tensor, torch.tensor]:\n",
    "        logits = self.token_embedding_table(idx)  # B T C\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            # Torch expects B C T\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], 1)\n",
    "        return idx\n",
    "    \n",
    "m = BigramLangugeModel(vocab_size)\n",
    "out, loss= m(xb, yb)\n",
    "print(out.shape)\n",
    "print(loss)\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4964585304260254\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for _ in range(20000):\n",
    "    # get batch\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pes isathod? Hinvearet eoftre:\n",
      "\n",
      "\n",
      "Cich; mary?\n",
      "And fein, k whear! w kenendwarend:\n",
      "en's,\n",
      "LI arse wristhendsok. ghen. por t tucofer I:\n",
      "Ore HINGau, itst manon: a worf myste t brway cathedse scoereayonck k'BOLLUFandoo-ksele whithigreflsucct iaut be tet d nt hon' be INGrr'lik!\n",
      "HIth ss hakime r tene ch awic\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(13)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t + 1]\n",
    "        xbow[b, t] = xprev.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones((T, T)))\n",
    "wei /= wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tri == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "xbow3 = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow3, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre Self Attention\n",
    "torch.manual_seed(13)\n",
    "# Initialize an average batch, that transformer gets\n",
    "# B - batch, T - time, C - channel\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Lower triangular matrix\n",
    "tri = torch.tril(torch.ones(T, T))\n",
    "# Matrix for future weights\n",
    "wei = torch.zeros((T, T))\n",
    "# Future doesn't communicate with the past!\n",
    "# Tokens are unable to gather any info from the following characters in a batch\n",
    "# Use triangular matrix to mask anything, that comes after the token as '-inf'\n",
    "# e**(-inf) ---> 0.0, and that's what future caracters data will be multiplied by\n",
    "wei = wei.masked_fill(tri == 0, float('-inf'))\n",
    "# Softmax performs elemet-wise exponentiation and along-rows normalization\n",
    "# Here's your weights\n",
    "wei = F.softmax(wei, dim=1)\n",
    "\n",
    "out = wei @ x\n",
    "out.shape\n",
    "# What this code does at the end is putting all date from past and simply averaging it\n",
    "# This results in a very lossy connection between and cuts the most part of information\n",
    "# This happens because values in triangle matrix were hard-coded as one, but \n",
    "# Self Attention approach allows us to make this number complition data dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self Attention (single step missing)\n",
    "torch.manual_seed(13)\n",
    "# B - batch, T - time, C - channel\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Dimensianality of key and query vectors\n",
    "head_size = 16\n",
    "# Linear layers, that form key and a query from token's identity\n",
    "key = nn.Linear(C, head_size, bias=False)    # (C, head_size)\n",
    "query = nn.Linear(C, head_size, bias=False)  # (C, head_size)\n",
    "# Create keys and querys for all tokens in a batch\n",
    "k = key(x)                                # (B, T, head_size)\n",
    "q = query(x)                              # (B, T, head_size)\n",
    "# Inintialize our weights with query @ key.T products instead of zeroes\n",
    "wei = q @ k.transpose(-2, -1)             # (B, T, T)\n",
    "\n",
    "tri = torch.tril(torch.ones((T, T)))\n",
    "# Future doesn't communicate with the past!\n",
    "wei = wei.masked_fill(tri == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "out.shape\n",
    "# Weighted aggregation now is a function in a data dependent manner \n",
    "# between keys and querys of tokens(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3347, grad_fn=<VarBackward0>)\n",
      "tensor(0.3802, grad_fn=<VarBackward0>)\n",
      "tensor(0.1373, grad_fn=<VarBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self Attention\n",
    "torch.manual_seed(13)\n",
    "# B - batch, T - time, C - channel\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Dimensianality of key and query vectors\n",
    "head_size = 16\n",
    "# Linear layers, that form key and a query from token's identity\n",
    "key = nn.Linear(C, head_size, bias=False)    # (C, head_size)\n",
    "query = nn.Linear(C, head_size, bias=False)  # (C, head_size)\n",
    "\n",
    "# Create keys and querys for all tokens in a batch\n",
    "k = key(x)                                # (B, T, head_size)\n",
    "q = query(x)                              # (B, T, head_size)\n",
    "\n",
    "# Inintialize our weights with query @ key.T products instead of zeroes\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5   # (B, T, T)\n",
    "\n",
    "tri = torch.tril(torch.ones((T, T)))\n",
    "# Future doesn't communicate with the past!\n",
    "wei = wei.masked_fill(tri == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "\n",
    "value = nn.Linear(C, head_size, bias=False)  # (C, head_size)\n",
    "v = value(x)                              # (B, T, head_size)\n",
    "# Instead of the raw \"x\",  \"v\" vector is what we aggregate.\n",
    "# \"v\" comes from \"value\" obtained the same as \"key\", \"query\"\n",
    "out = wei @ v\n",
    "out.shape\n",
    "# Weighted aggregation now is a function in a data dependent manner \n",
    "# between keys and querys of tokens(nodes)\n",
    "\n",
    "# \"x\" is information, private to a token.\n",
    "# I'm a fifth token, I have some identity, and my information is kept in vector \"x\".\n",
    "# Here's what I'm intersted in (\"query\"), here's what I have (\"key\"), and if you find\n",
    "# me interesting here's what I'll communcate to you (value).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3646, 0.6354, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3106, 0.4625, 0.2269, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2624, 0.2414, 0.2755, 0.2207, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1727, 0.2276, 0.2303, 0.2185, 0.1509, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1618, 0.1240, 0.2014, 0.1921, 0.2105, 0.1101, 0.0000, 0.0000],\n",
       "         [0.2054, 0.2259, 0.1083, 0.1471, 0.1160, 0.1014, 0.0959, 0.0000],\n",
       "         [0.1327, 0.0882, 0.1372, 0.0856, 0.1931, 0.1874, 0.0747, 0.1011]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3861, 0.6139, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4788, 0.2497, 0.2715, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2468, 0.1966, 0.2727, 0.2838, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2278, 0.2794, 0.2087, 0.1140, 0.1701, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2236, 0.2194, 0.1887, 0.1171, 0.1434, 0.1078, 0.0000, 0.0000],\n",
       "         [0.1714, 0.1886, 0.0936, 0.1288, 0.1711, 0.1358, 0.1108, 0.0000],\n",
       "         [0.1060, 0.1641, 0.1783, 0.0955, 0.1431, 0.1044, 0.1346, 0.0740]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4593, 0.5407, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1857, 0.2569, 0.5574, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1846, 0.1986, 0.3988, 0.2180, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1051, 0.1512, 0.3521, 0.1898, 0.2019, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3118, 0.1665, 0.0857, 0.1252, 0.0843, 0.2265, 0.0000, 0.0000],\n",
       "         [0.1121, 0.1159, 0.1314, 0.1419, 0.1845, 0.1631, 0.1511, 0.0000],\n",
       "         [0.0583, 0.0399, 0.3015, 0.0967, 0.1547, 0.0707, 0.1612, 0.1171]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4982, 0.5018, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3912, 0.3121, 0.2967, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1577, 0.3017, 0.2060, 0.3346, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1053, 0.2181, 0.1997, 0.3031, 0.1738, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2495, 0.1216, 0.2577, 0.1113, 0.1601, 0.0998, 0.0000, 0.0000],\n",
       "         [0.2959, 0.1370, 0.1529, 0.1051, 0.1332, 0.1130, 0.0629, 0.0000],\n",
       "         [0.1128, 0.0625, 0.1637, 0.0761, 0.0810, 0.0966, 0.1601, 0.2471]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(13)\n",
    "batch_size = 32\n",
    "\n",
    "# What this thing does is just an average communication, aggregating raw x values\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd: int) -> None:\n",
    "        # Inherit parent class behaviour\n",
    "        super().__init__()\n",
    "        # Create embedding tables\n",
    "        self.identity_embedding_table = nn.Embedding(vocab_size, n_embd)  # \"vocab_size\" - global\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # \"block_size\" - global\n",
    "        # Layer, converting prediction of \"n_embd\" length to logits of \"vocab_size\" length\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor = None) -> torch.tensor: # x - is a batch of shape (B, T)   \n",
    "        # Unpack batch dimensions\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Embed identities of each tolen in a batch\n",
    "        identity_embeddings = self.identity_embedding_table(idx)\n",
    "        # Embed positioins of each tolen in a batch\n",
    "        positions = torch.tile(torch.arange(T), (B, 1))\n",
    "        position_embeddings = self.position_embedding_table(positions)\n",
    "        # Combine that information\n",
    "        x = identity_embeddings + position_embeddings\n",
    "        # Averaging communication\n",
    "        tril = torch.tril(torch.ones(T, T))\n",
    "        wei = torch.masked_fill(tril, tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=1)\n",
    "        # \"out is a matrix of shape (B, T, C), where C is \"n_ebmd\"\n",
    "        out = wei @ x\n",
    "        # convert \"out\" to (B, T, C), where C is \"vocab_size\"\n",
    "        logits = self.lm_head(out)\n",
    "    \n",
    "        # Compute \"loss\", if necessary\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # F.cross_entropy expects input of shape (N, C), where N is a batch size\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # Pass two batch dimension we had as one\n",
    "            targets = targets.view(B * T)   # Reshape targets accordingly\n",
    "            loss = F.cross_entropy(logits, targets) if targets is not None else None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int = 100) -> None:\n",
    "        # \"idx\" is tensor of shape (4, 8)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Cut \"idx\" to the length of \"block_size\" in second dimension\n",
    "            forward_idx = idx[:, -block_size:] if idx.shape[-1] > block_size else idx\n",
    "            # Forward pass\n",
    "            logits, loss = self(forward_idx)\n",
    "            # (4, 8, 32) -> (4, 32)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Normlize last dimension\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Select single token for each vector of probabilitiess\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)  # (4, 1)\n",
    "            # (4, 8) cat (4, 1) ---> (4, 9)\n",
    "            idx = torch.cat([idx, next_idx], dim=1)\n",
    "        return idx\n",
    "\n",
    "mymodel = BigramLanguageModel(vocab_size)\n",
    "# inpt = torch.randint(vocab_size, (batch_size, block_size))\n",
    "# tgts = torch.randint(vocab_size, (batch_size, block_size))\n",
    "# res = mymodel(inpt, tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = tensor(4.3319, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(3.0761, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.8579, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.9286, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.7695, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.6387, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.9360, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.9126, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.8196, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.7113, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.8126, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.8382, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.9016, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.7930, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.6736, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.7699, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.7949, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.7944, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.8318, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.7495, grad_fn=<NllLossBackward0>)\n",
      "2.897916078567505\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(params=mymodel.parameters(), lr=1e-3)\n",
    "for i in range(20000):\n",
    "    # Sample batch\n",
    "    Xb, Yb = get_batch('train')\n",
    "    # Evaluate loss\n",
    "    logits, loss = mymodel(Xb, Yb)\n",
    "    # Clean gradients\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{loss = }\")\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Toushtll erash:\n",
      "y ytoh\n",
      "oe  Dayithyou s boylogfer  om isfift theotl. \n",
      "\n",
      " rIvols e tsam aha\n",
      "ndrt\n",
      "r e wndiosd mvpoiinol,e,  datne e hrasiadt n\n",
      "firsneo   imdfhgienu:  lc\n",
      "isakpened\n",
      "w sbela'hipr\n",
      "ptecnEoi\n",
      "r tld hior i a lobfrhle!yI wo  nnitdwos.\n",
      "-Ca\n",
      "KNTuon ygoowb\n",
      "iturcaahdr'ds\n",
      " hasmiiek :ns' gss\n",
      "Ii Pveulano\n"
     ]
    }
   ],
   "source": [
    "tokens = mymodel.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(13)\n",
    "batch_size = 32\n",
    "head_size = 16\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements behaviour of single Self-Attention head. Given batch of token's \n",
    "    identities does the \"query - key - value\" encoded block and returns aggregated value \n",
    "    for each token in a batch, already converted to original dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int) -> None:\n",
    "        # Inherit parent class behaviour\n",
    "        super().__init__()\n",
    "        # input tensor of shape (B, T, C), where \"C\" is n_embd -> (B, T, head_size)\n",
    "        # \"head_size\" - global\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)  \n",
    "        self.key = nn.Linear(n_embd,  head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd,  head_size, bias=False)\n",
    "\n",
    "        # Layer, converting aggregated value of \"head_size\" length to raw \"x\" of \"n_embd\" length\n",
    "        self.to_embed_size = nn.Linear(head_size, n_embd, bias=False)  # \"head_size\" - global\n",
    "\n",
    "    def forward(self, idx: torch.tensor) -> None:\n",
    "        B, T, C = idx.shape\n",
    "        # First we get querys, keys and values from \"idx\"\n",
    "        q = self.query(idx)  # (B, T, head_size)\n",
    "        k = self.key(idx)    # (B, T, head_size)\n",
    "        v = self.value(idx)  # (B, T, head_size)\n",
    "        # Next, we get dot products of q and k\n",
    "        # (B, T, head_size) @ (B, head_size, T) ---> (B, T, T)\n",
    "        wei = q @ k.transpose(-1, -2) * head_size**-0.5\n",
    "        # Mask future tokens\n",
    "        tril = torch.tril(torch.ones((T, T)))\n",
    "        wei = torch.masked_fill(wei, tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=1)\n",
    "        # Aggregate values\n",
    "        # (B, T, T) @ (B, T, head_size) ---> (B, T, head_size)\n",
    "        out = wei @ v\n",
    "        # Convert \"out\" to (B, T, n_embd)\n",
    "        out = self.to_embed_size(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# What this thing does is just an average communication, aggregating raw x values. Let's change it.\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd: int) -> None:\n",
    "        # Inherit parent class behaviour\n",
    "        super().__init__()\n",
    "        # Create embedding tables\n",
    "        self.identity_embedding_table = nn.Embedding(vocab_size, n_embd)  # \"vocab_size\" - global\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # \"block_size\" - global\n",
    "        # Layer, converting raw \"x\" of \"n_embd\" length to logits of \"vocab_size\" length\n",
    "        self.to_logit_size = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        # Self-attention head\n",
    "        self.sa_head = Head(n_embd)\n",
    "        \n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor = None) -> torch.tensor: # x - is a batch of shape (B, T)   \n",
    "        # Unpack batch dimensions\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Embed identities of each tolen in a batch\n",
    "        identity_embeddings = self.identity_embedding_table(idx)\n",
    "        # Embed positioins of each tolen in a batch\n",
    "        positions = torch.tile(torch.arange(T), (B, 1))\n",
    "        position_embeddings = self.position_embedding_table(positions)\n",
    "        # Combine that information\n",
    "        x = identity_embeddings + position_embeddings\n",
    "        # Call self-attention head\n",
    "        out = self.sa_head(x)\n",
    "\n",
    "        # convert \"out\" to (B, T, C), where C is \"vocab_size\"\n",
    "        logits = self.to_logit_size(out)\n",
    "    \n",
    "        # Compute \"loss\", if necessary\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # F.cross_entropy expects input of shape (N, C), where N is a batch size\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # Pass two batch dimension we had as one\n",
    "            targets = targets.view(B * T)   # Reshape targets accordingly\n",
    "            loss = F.cross_entropy(logits, targets) if targets is not None else None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int = 100) -> None:\n",
    "        # \"idx\" is tensor of shape (4, 8)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Cut \"idx\" to the length of \"block_size\" in second dimension\n",
    "            forward_idx = idx[:, -block_size:] if idx.shape[-1] > block_size else idx\n",
    "            # Forward pass\n",
    "            logits, loss = self(forward_idx)\n",
    "            # (4, 8, 32) -> (4, 32)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Normlize last dimension\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Select single token for each vector of probabilitiess\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)  # (4, 1)\n",
    "            # (4, 8) cat (4, 1) ---> (4, 9)\n",
    "            idx = torch.cat([idx, next_idx], dim=1)\n",
    "        return idx\n",
    "\n",
    "mymodel = BigramLanguageModel(vocab_size)\n",
    "# inpt = torch.randint(vocab_size, (batch_size, block_size))\n",
    "# tgts = torch.randint(vocab_size, (batch_size, block_size))\n",
    "# res = mymodel(inpt, tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = tensor(4.2009, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.3883, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1538, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1442, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.3113, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.2480, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.2062, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.3446, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.3126, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1889, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.2682, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1976, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1170, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1344, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1019, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1759, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1822, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1842, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.2563, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.2497, grad_fn=<NllLossBackward0>)\n",
      "1.9951215982437134\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(params=mymodel.parameters(), lr=1e-3)\n",
    "for i in range(20000):\n",
    "    # Sample batch\n",
    "    Xb, Yb = get_batch('train')\n",
    "    # Evaluate loss\n",
    "    logits, loss = mymodel(Xb, Yb)\n",
    "    # Clean gradients\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{loss = }\")\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "OMENENENY:\n",
      "Mese INI thoupr thexel.\n",
      "\n",
      "BENIvilll.\n",
      "\n",
      "LINGRENGROx:\n",
      "S ININENINGERNYCHENTENINCILANGRADYO:\n",
      "Dind, wof rs es avild hy bus: mece;\n",
      "ARY:\n",
      "Wacind meackind\n",
      "Cancomonde the be ror ando, I thay. won ndind,\n",
      "And-\n",
      "Wigicuthel'll?\n",
      "\n",
      "OMENENCIANGLER:\n",
      "Wemisean:\n",
      "Ang.\n",
      "\n",
      "\n",
      "Sikn theanot.\n",
      "\n",
      "SY:\n",
      "Bure prod fiveve ty I st\n"
     ]
    }
   ],
   "source": [
    "tokens = mymodel.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(13)\n",
    "batch_size = 32\n",
    "n_embd = 32\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements behaviour of single Self-Attention head. Given batch of token's \n",
    "    identities does the \"query - key - value\" encoded block and returns aggregated value \n",
    "    for each token in a batch, already converted to original dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_size: int) -> None:\n",
    "        # Inherit parent class behaviour\n",
    "        super().__init__()\n",
    "        # input tensor of shape (B, T, C), where \"C\" is n_embd -> (B, T, head_size)\n",
    "        # \"n_embd\" - global\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)  \n",
    "        self.key = nn.Linear(n_embd,  head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd,  head_size, bias=False)\n",
    "\n",
    "        # Store masking matrix as a buffer which is not a parameter oof the model\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, idx: torch.tensor) -> None:\n",
    "        B, T, C = idx.shape\n",
    "        # First we get querys, keys and values from \"idx\"\n",
    "        q = self.query(idx)  # (B, T, head_size)\n",
    "        k = self.key(idx)    # (B, T, head_size)\n",
    "        v = self.value(idx)  # (B, T, head_size)\n",
    "        # Next, we get dot products of q and k\n",
    "        # (B, T, head_size) @ (B, head_size, T) ---> (B, T, T)\n",
    "        wei = q @ k.transpose(-1, -2) * head_size**-0.5  # AFTER_DEBUG: C instead of head_size !\n",
    "        # Mask future tokens\n",
    "        wei = torch.masked_fill(wei, self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=1)\n",
    "        # Aggregate values\n",
    "        # (B, T, T) @ (B, T, head_size) ---> (B, T, head_size)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "# What this thing does is just an average communication, aggregating raw x values. Let's change it.\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        # Inherit parent class behaviour\n",
    "        super().__init__()\n",
    "        # Create embedding tables\n",
    "        self.identity_embedding_table = nn.Embedding(vocab_size, n_embd)  # \"vocab_size\" - global\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # \"block_size\" - global\n",
    "        # Layer, converting raw \"x\" of \"n_embd\" length to logits of \"vocab_size\" length\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        # Self-attention head\n",
    "        self.sa_head = Head(n_embd)\n",
    "        \n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor = None) -> torch.tensor: # x - is a batch of shape (B, T)   \n",
    "        # Unpack batch dimensions\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Embed identities of each tolen in a batch\n",
    "        identity_embeddings = self.identity_embedding_table(idx)\n",
    "        # Embed positioins of each tolen in a batch\n",
    "        positions = torch.tile(torch.arange(T), (B, 1))\n",
    "        position_embeddings = self.position_embedding_table(positions)\n",
    "        # Combine that information\n",
    "        x = identity_embeddings + position_embeddings\n",
    "        # Call self-attention head\n",
    "        x = self.sa_head(x)\n",
    "        # convert \"out\" to (B, T, C), where C is \"vocab_size\"\n",
    "        logits = self.lm_head(x)\n",
    "    \n",
    "        # Compute \"loss\", if necessary\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # F.cross_entropy expects input of shape (N, C), where N is a batch size\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # Pass two batch dimension we had as one\n",
    "            targets = targets.view(B * T)   # Reshape targets accordingly\n",
    "            loss = F.cross_entropy(logits, targets) if targets is not None else None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int = 100) -> None:\n",
    "        # \"idx\" is tensor of shape (4, 8)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Cut \"idx\" to the length of \"block_size\" in second dimension\n",
    "            forward_idx = idx[:, -block_size:] if idx.shape[-1] > block_size else idx\n",
    "            # Forward pass\n",
    "            logits, loss = self(forward_idx)\n",
    "            # (4, 8, 32) -> (4, 32)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Normlize last dimension\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Select single token for each vector of probabilitiess\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)  # (4, 1)\n",
    "            # (4, 8) cat (4, 1) ---> (4, 9)\n",
    "            idx = torch.cat([idx, next_idx], dim=1)\n",
    "        return idx\n",
    "\n",
    "mymodel = BigramLanguageModel()\n",
    "# inpt = torch.randint(vocab_size, (batch_size, block_size))\n",
    "# tgts = torch.randint(vocab_size, (batch_size, block_size))\n",
    "# res = mymodel(inpt, tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = tensor(4.3044, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.4063, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.2833, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.3781, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.2818, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1793, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.3244, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.3041, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.2804, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1504, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.2674, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.3244, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.3394, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1236, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1418, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.0876, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1257, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.2508, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.3066, grad_fn=<NllLossBackward0>)\n",
      "loss = tensor(2.1487, grad_fn=<NllLossBackward0>)\n",
      "2.19347882270813\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(params=mymodel.parameters(), lr=1e-3)\n",
    "for i in range(20000):\n",
    "    # Sample batch\n",
    "    Xb, Yb = get_batch('train')\n",
    "    # Evaluate loss\n",
    "    logits, loss = mymodel(Xb, Yb)\n",
    "    # Clean gradients\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{loss = }\")\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NI:\n",
      "Shindy con my.\n",
      "\n",
      "MAUD ho bourg bred balld,\n",
      "Cougveal this.\n",
      "CAng fo seat-'my harely me cant latherg ods wid th, in?\n",
      "ANY:\n",
      "Longzomend ow ith met ars wird thes pe be nd aving, thowre, ye orw anonis this-thoushit mat irt hes at borls foug ace on ty for:\n",
      "Voly thal is pur aly rim?\n",
      "\n",
      "And to for'd ngil thed\n"
     ]
    }
   ],
   "source": [
    "tokens = mymodel.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Networks suffer from normaliztion issues and we face this stuff we our net too.\n",
    "# Let's introduce two techniques to fight this stuff:\n",
    "# 1) Residual connnections. We're simply adding any caomputation dine to what we have before it.\n",
    "# 2) Layer normalization, not a batch normalization, but it does the same thing, but along the rows.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(13)\n",
    "batch_size = 32\n",
    "n_embd = 32\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements behaviour of single Self-Attention head. Given batch of token's \n",
    "    identities does the \"query - key - value\" encoded block and returns aggregated value \n",
    "    for each token in a batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_size: int) -> None:\n",
    "        # Inherit parent class behaviour\n",
    "        super().__init__()\n",
    "        # input tensor of shape (B, T, C), where \"C\" is n_embd -> (B, T, head_size)\n",
    "        # \"n_embd\" - global\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)  \n",
    "        self.key = nn.Linear(n_embd,  head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd,  head_size, bias=False)\n",
    "\n",
    "        # Store masking matrix as a buffer which is not a parameter oof the model\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, idx: torch.tensor) -> torch.tensor:\n",
    "        B, T, C = idx.shape\n",
    "        # First we get querys, keys and values from \"idx\"\n",
    "        q = self.query(idx)  # (B, T, head_size)\n",
    "        k = self.key(idx)    # (B, T, head_size)\n",
    "        v = self.value(idx)  # (B, T, head_size)\n",
    "        # Next, we get dot products of q and k\n",
    "        # (B, T, head_size) @ (B, head_size, T) ---> (B, T, T)\n",
    "        wei = q @ k.transpose(-1, -2) * head_size**-0.5  # AFTER_DEBUG: C instead of head_size !\n",
    "        # Mask future tokens\n",
    "        wei = torch.masked_fill(wei, self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=1)\n",
    "        # Aggregate values\n",
    "        # (B, T, T) @ (B, T, head_size) ---> (B, T, head_size)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This class initializes given amount of Self-Attention heads of given head size, runs\n",
    "    all of them in parallel and returns concatemation of their work along channel dimension.\n",
    "    Output's shape is (B, T, C), where C is \"num_heads\" * \"head_size\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # Residual connection layer\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # Project what we've calculated, or control it's influence  \n",
    "        # since self.proj is a trainable parameter\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements thinking process, where by thinking I mean simple MLP neural net.\n",
    "    Given tensor of n_embd passes it through Linear layer and ReLU, returning tensor of original shape.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int) -> None:\n",
    "        # Inherit behaviour of parent class\n",
    "        super().__init__()\n",
    "        # Mini-net of linear activation followed by ReLU activation\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "            # Residual connection layer\n",
    "            # This is the way of control which parts of computation \n",
    "            # we did affect original data more and which are less\n",
    "            nn.Linear(n_embd, n_embd)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        # Forwrad mini-net\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    This class just combines talking and thinking stages, where talking \n",
    "    is multi-head self-attention and thinking is a simple MLP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, num_heads: int) -> None:\n",
    "        # Inherit behaviour of parent class\n",
    "        super().__init__()\n",
    "        head_size = n_embd // num_heads\n",
    "        self.sa = MultiHeadAttention(num_heads, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection: add result of computation to the input\n",
    "        x = x + self.sa(self.ln1(x))    # first add communication part\n",
    "        x = x + self.ffwd(self.ln2(x))  # then add computation part\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        # Inherit parent class behaviour\n",
    "        super().__init__()\n",
    "        # Create embedding tables\n",
    "        self.identity_embedding_table = nn.Embedding(vocab_size, n_embd)  # \"vocab_size\" - global\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # \"block_size\" - global\n",
    "        # Layer, converting raw \"x\" of \"n_embd\" length to logits of \"vocab_size\" length\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        # Stack multiple blocks upon each other\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, num_heads=4),\n",
    "            Block(n_embd, num_heads=4),\n",
    "            Block(n_embd, num_heads=4),\n",
    "            # Layer norm after transformer right before \"lm_head\"\n",
    "            nn.LayerNorm(n_embd)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor = None) -> torch.tensor: # x - is a batch of shape (B, T)   \n",
    "        # Unpack batch dimensions\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Embed identities of each tolen in a batch\n",
    "        identity_embeddings = self.identity_embedding_table(idx)\n",
    "        # Embed positioins of each tolen in a batch\n",
    "        positions = torch.tile(torch.arange(T), (B, 1))\n",
    "        position_embeddings = self.position_embedding_table(positions)\n",
    "        # Combine that information\n",
    "        x = identity_embeddings + position_embeddings\n",
    "        # Call all \"talk-think\" blocks, that's the forward pass of the model now\n",
    "        x = self.blocks(x)\n",
    "        # convert \"out\" to (B, T, C), where C is \"vocab_size\"\n",
    "        logits = self.lm_head(x)\n",
    "    \n",
    "        # Compute \"loss\", if necessary\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # F.cross_entropy expects input of shape (N, C), where N is a batch size\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # Pass two batch dimension we had as one\n",
    "            targets = targets.view(B * T)   # Reshape targets accordingly\n",
    "            loss = F.cross_entropy(logits, targets) if targets is not None else None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int = 100) -> None:\n",
    "        # \"idx\" is tensor of shape (4, 8)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Cut \"idx\" to the length of \"block_size\" in second dimension\n",
    "            forward_idx = idx[:, -block_size:] if idx.shape[-1] > block_size else idx\n",
    "            # Forward pass\n",
    "            logits, loss = self(forward_idx)\n",
    "            # (4, 8, 32) -> (4, 32)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Normlize last dimension\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Select single token for each vector of probabilitiess\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)  # (4, 1)\n",
    "            # (4, 8) cat (4, 1) ---> (4, 9)\n",
    "            idx = torch.cat([idx, next_idx], dim=1)\n",
    "        return idx\n",
    "\n",
    "mymodel = BigramLanguageModel()\n",
    "# inpt = torch.randint(vocab_size, (batch_size, block_size))\n",
    "# tgts = torch.randint(vocab_size, (batch_size, block_size))\n",
    "# res = mymodel(inpt, tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = tensor(1.2882, grad_fn=<NllLossBackward0>)\n",
      "0.45698636770248413\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(params=mymodel.parameters(), lr=1e-3)\n",
    "for i in range(1000):\n",
    "    # Sample batch\n",
    "    Xb, Yb = get_batch('train')\n",
    "    # Evaluate loss\n",
    "    logits, loss = mymodel(Xb, Yb)\n",
    "    # Clean gradients\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{loss = }\")\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ODO\n",
      "RAnd.\n",
      "\n",
      "Bederaraparearsencerw Huasch fey abey dy amateand, re?\n",
      "\n",
      "BENENCENELESTAOtud as th, suchife isuchowhed ferenest; ats we to ont, aich thor inuenf, harest, enceien owhir, warket-\n",
      "LO, fo, ange elet toisecharref ast ned ughourmem sus wo dod, I sw he do do ther ind, foruch sewe thareedee fery wair cAnum fip', I my Ray, th ifocudsong, I tellifl, wovod.\n",
      "Andy parebpire yret by rin noto oouill wor, I Sret obtur fo.\n",
      "NRHINUOMLAnd thy hepl,\n",
      "Nown turd. ICE cegt,\n",
      "TBAreps, no,\n",
      "Ya thadel the rown;\n",
      "HYo\n"
     ]
    }
   ],
   "source": [
    "tokens = mymodel.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
