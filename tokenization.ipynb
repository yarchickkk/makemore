{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1071, 32, 1093, 1086, 1095, 1091, 32, 1091, 1095, 1080, 1090, 1100, 32, 1092, 1088, 1072, 1085, 1094, 1091, 1079, 1089, 1082, 1080, 1081, 33]\n"
     ]
    }
   ],
   "source": [
    "# Unicode\n",
    "h = 'Я хочу учить французский!'\n",
    "print([ord(l) for l in h])\n",
    "# Vocabulary is big (~150k) and it's still alive and keeps changing.\n",
    "# Having constant text representation is preferrable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208, 175, 32, 209, 133, 208, 190, 209, 135, 209, 131, 32, 209, 131, 209, 135, 208, 184, 209, 130, 209, 140, 32, 209, 132, 209, 128, 208, 176, 208, 189, 209, 134, 209, 131, 208, 183, 209, 129, 208, 186, 208, 184, 208, 185, 33]\n"
     ]
    }
   ],
   "source": [
    "# Encodings turn unicode codepoints (unique integers) \n",
    "# and turn them to sequence from 1 to 4 bytes long.\n",
    "# No matter what state Unicode is at the moment,\n",
    "# shape of \"utf-8\" remains the same.\n",
    "byteobject = h.encode(\"utf-8\")\n",
    "integers = list(byteobject)\n",
    "print(integers)\n",
    "# Vocabulary size is small now what results in longer text represenation.\n",
    "# Since we work on bathes, bigger size of one is needed for considering same context.\n",
    "# Bigger batch increases computational cost of attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40))]\n"
     ]
    }
   ],
   "source": [
    "# Instead we want to support bigger vocabulary size, that we can tune as a hyperparameter.\n",
    "# But at the same time to stick to raw bytes \"utf-8\" representation. \n",
    "string = 'Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.'\n",
    "tokens = list(string.encode('utf-8'))\n",
    "\n",
    "def get_stats(ids: list) -> dict:\n",
    "    stats = dict()\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        stats[pair] = stats.get(pair, 0) + 1\n",
    "    return stats\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(sorted(((v, k) for k, v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge(ids: list, target_pair: tuple, new_token: int) -> list:\n",
    "    new_ids, i = [], 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and (ids[i], ids[i+1]) == target_pair:\n",
    "            new_ids.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "res = merge(tokens, max(stats, key=stats.get), 256)\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length in characters: 22191\n",
      "Length in bytes:      23431\n"
     ]
    }
   ],
   "source": [
    "# Read the whole article\n",
    "with open('text.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "print('Length in characters:', len(text))\n",
    "tokens = list(text.encode('utf-8'))\n",
    "print('Length in bytes:     ', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair (101, 32) gets merged to --> 256\n",
      "pair (105, 110) gets merged to --> 257\n",
      "pair (115, 32) gets merged to --> 258\n",
      "pair (116, 104) gets merged to --> 259\n",
      "pair (101, 114) gets merged to --> 260\n",
      "pair (116, 32) gets merged to --> 261\n",
      "pair (99, 111) gets merged to --> 262\n",
      "pair (226, 128) gets merged to --> 263\n",
      "pair (44, 32) gets merged to --> 264\n",
      "pair (97, 110) gets merged to --> 265\n",
      "pair (111, 114) gets merged to --> 266\n",
      "pair (100, 32) gets merged to --> 267\n",
      "pair (97, 114) gets merged to --> 268\n",
      "pair (101, 110) gets merged to --> 269\n",
      "pair (257, 103) gets merged to --> 270\n",
      "pair (262, 100) gets merged to --> 271\n",
      "pair (121, 32) gets merged to --> 272\n",
      "pair (259, 256) gets merged to --> 273\n",
      "pair (97, 108) gets merged to --> 274\n",
      "pair (111, 110) gets merged to --> 275\n",
      "-------------------------------------------------------\n",
      "before: 23431 | after: 18487 | compression ratio: 1.27x\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 276  # desired final voacabulary size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)  # deepcopy to save original tokens\n",
    "\n",
    "merges = {}  # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f'pair {pair} gets merged to --> {idx}')\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n",
    "\n",
    "print(f'{\"-\"*55}\\nbefore: {len(tokens)} | after: {len(ids)} | compression ratio: {len(tokens) / len(ids):.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Here's my implementation, not the prettiest in the world, but it works\n",
    "def my_decode(sequence: list, vocab_size: int, decodings: dict) -> list:\n",
    "    \"\"\"\n",
    "    Sequence of tokens, [0; vocab_size] ---> Sequence of bytes, [0; vocab_size]\n",
    "    It's like a reverse transform. Running loop decode token pair at iteration.\n",
    "    \"\"\"\n",
    "    # invert decodings dictionary since it's values are unique\n",
    "    decodings = {v: k for k, v in decodings.items()}  # int -> (int, int)\n",
    "    \n",
    "    for token in range(vocab_size, 255, -1):\n",
    "        state = []\n",
    "        # iterate over all tokens being decoded\n",
    "        for elem in sequence:\n",
    "            # iterate over given sequence\n",
    "            if elem == token:\n",
    "                state.extend(decodings[token])\n",
    "            else:\n",
    "                state.append(elem)\n",
    "        # point state of sequence after current token decoding\n",
    "        sequence = state\n",
    "    # integers -> bytes -> unicode characters \n",
    "    return bytes(sequence).decode('utf-8')\n",
    "\n",
    "res = my_decode(ids, vocab_size, merges)\n",
    "print(res == text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]  # concatenation of byte objects\n",
    "\n",
    "def decode(ids: list[int]) -> str:\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode('utf-8', errors='replace')\n",
    "    return text\n",
    "\n",
    "res2 = decode(ids)\n",
    "print(res2 == text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
